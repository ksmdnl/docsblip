{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93cbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DKusuma\\AppData\\Local\\miniconda3\\envs\\docsblip\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b555d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.models.docsblip import DocsBlip\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LayoutLMv3FeatureExtractor, LayoutLMv3Processor, LayoutLMv3TokenizerFast, LiltModel\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe405f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DKusuma\\AppData\\Local\\miniconda3\\envs\\docsblip\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:32: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DocsBlip()\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "lilt_tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n",
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=True)\n",
    "processor = LayoutLMv3Processor(feature_extractor, lilt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fabb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nielsr/funsd\", split=\"test\")\n",
    "sample = dataset[0]\n",
    "\n",
    "image = sample['image']\n",
    "width, height = image.size\n",
    "words = [w for w in sample[\"words\"] if len(w) > 0]\n",
    "bboxes = [b for b in sample[\"bboxes\"]][:len(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5de72b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73a19997",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = processor(image, return_offsets_mapping=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e4f4246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "attention_mask\n",
      "offset_mapping\n",
      "bbox\n",
      "pixel_values\n"
     ]
    }
   ],
   "source": [
    "for k, v in encoding.items(): print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15d0d802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc95f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(bbox, width, height):\n",
    "    \"\"\"Scale pixel coords to [0, 1000] range for LiLT\"\"\"\n",
    "    return [\n",
    "        int(1000 * bbox[0] / width),\n",
    "        int(1000 * bbox[1] / height),\n",
    "        int(1000 * bbox[2] / width),\n",
    "        int(1000 * bbox[3] / height),\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_funsd_sample():\n",
    "    dataset = load_dataset(\"nielsr/funsd\", split=\"test\")\n",
    "    sample = dataset[0]\n",
    "\n",
    "    image = sample['image']\n",
    "    width, height = image.size\n",
    "    words = [w for w in sample[\"words\"] if len(w) > 0]\n",
    "    # bboxes = [normalize_bbox(b, width, height) for b in sample[\"bboxes\"]][:len(words)]\n",
    "    bboxes = [b for b in sample[\"bboxes\"]][:len(words)]\n",
    "\n",
    "    return image, words, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b330d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58501bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instruction: What is the total amount?\n",
      "Answer:  Colonelshireshireshireadvertisementshireadvertisementadvertisementshireshire[/shireshire [/shireshire©shire[/ [/ Coloneladvertisement Colonelshireadvertisement © Colonelshire\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image, words, bboxes = load_funsd_sample()\n",
    "encoding = lilt_tokenizer(\n",
    "    text=words,\n",
    "    boxes=bboxes,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids = encoding.input_ids.to(device)\n",
    "attention_mask = encoding.attention_mask.to(device)\n",
    "bbox_tensor = encoding.bbox.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.encoder.model(\n",
    "        input_ids=input_ids,\n",
    "        bbox=bbox_tensor,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "hidden_states = encoder_outputs.last_hidden_state\n",
    "\n",
    "hidden_proj = BaseModelOutput(model.adapter(hidden_states))\n",
    "\n",
    "instr = \"What is the total amount?\"\n",
    "\n",
    "instr_tokens = tokenizer(\n",
    "    instr,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=30,\n",
    "    num_beams=3,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Generate tokens with a decoder given features\n",
    "with torch.no_grad():\n",
    "    outputs = model.decoder.generate(\n",
    "        encoder_outputs=hidden_proj,\n",
    "        input_ids=instr_tokens.input_ids,\n",
    "        attention_mask=instr_tokens.attention_mask,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(\"\\nInstruction:\", instr)\n",
    "print(\"Answer:\", generated[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docsblip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
